#!/bin/sh
# ì •ê¸° AI ìŠ¤ìº” ìŠ¤í¬ë¦½íŠ¸ (Docker ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ ì‹¤í–‰)
# ë§¤ì£¼ ì¼ìš”ì¼ì— ì‹¤í–‰ë¨
#
# v3.0 ì—…ë°ì´íŠ¸:
# - íŒŒì¼ ë‹¨ìœ„ ì €ì¥ (ì ìˆ˜ ë¬´ê´€, ëª¨ë“  íŒŒì¼ ì €ì¥)
# - í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ ì§€ì›
# - AI ë©”íƒ€ë°ì´í„° ë¶„ì„ ìœ ì§€
# - ëª¨ë“  í´ë” ìŠ¤ìº” (pages, components, composables, etc.)

echo "========================================="
echo "ğŸ¤– Weekly File-Based AI Scan v3.0"
echo "ğŸ“… $(date)"
echo "========================================="

cd /app

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export LLM_MODEL="${LLM_MODEL:-qwen2.5-coder:7b}"
export CONCURRENCY="${CONCURRENCY:-2}"
export BESTCASE_STORAGE_PATH="${BESTCASE_STORAGE_PATH:-/projects/.bestcases}"
export MAX_FILES_PER_PROJECT="${MAX_FILES_PER_PROJECT:-50}"
export OLLAMA_URL="${OLLAMA_URL:-http://ollama:11434}"

# Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
if ! curl -sf http://ollama:11434/api/tags > /dev/null 2>&1; then
  echo "âš ï¸ Ollama server not available, skipping AI scan"
  echo "Please check Ollama container status"
  exit 1
fi

echo "âœ… Ollama available, starting weekly scan process"
echo "ğŸ§  LLM Model: $LLM_MODEL"
echo "âš¡ Concurrency: $CONCURRENCY"
echo "ğŸ“ Storage: $BESTCASE_STORAGE_PATH"
echo "ğŸ“Š Max files/project: $MAX_FILES_PER_PROJECT"
echo ""

# 1. ë²„ì „ ì²´í¬ (ë“œë¼ì´ ëŸ°)
echo "ğŸ“Š Phase 1: Checking BestCase/FileCase versions..."
node --experimental-specifier-resolution=node /app/scripts/dist/scan/migrate-bestcases.js --dry-run 2>/dev/null || true
echo ""

# 2. AI ê¸°ë°˜ íŒŒì¼ ë‹¨ìœ„ ìŠ¤ìº” (v3.0)
echo "ğŸ” Phase 2: Running AI file-based scan..."
node --experimental-specifier-resolution=node /app/scripts/dist/scan/scan-files-ai.js

echo ""
echo "âœ¨ Weekly AI scan completed at $(date)"
echo "   - All files saved individually (no score filtering)"
echo "   - Keywords extracted for search"
echo "   - Individual scores recorded (no weighted total)"
echo ""
