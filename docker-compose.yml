version: '3.8'

services:
  # Ollama LLM Server for Code Analysis with GPU
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-code-analyzer
    volumes:
      - ollama-models:/root/.ollama
    ports:
      - "11434:11434"
    restart: unless-stopped
    networks:
      - mcp-network
    # GPU 활성화 (NVIDIA GPU가 있는 경우)
    # 없으면 CPU로 동작
    runtime: nvidia
    deploy:
      resources:
        limits:
          memory: 24G
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['1']
              capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - OLLAMA_NUM_PARALLEL=3
      - OLLAMA_MAX_LOADED_MODELS=1
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

  # MCP Code Mode Server
  mcp-code-mode:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: mcp-code-mode-server
    ports:
      - "3000:3000"
    volumes:
      - D:/01.Work/01.Projects:/projects
    environment:
      - NODE_ENV=production
      - PROJECTS_PATH=/projects
      - BESTCASE_STORAGE_PATH=/projects/.bestcases
      - OLLAMA_URL=http://ollama:11434
      - OLLAMA_HOST=http://ollama:11434
      - LLM_PROVIDER=ollama
      - LLM_MODEL=qwen2.5-coder:7b
      - CONCURRENCY=2
    deploy:
      resources:
        limits:
          memory: 8G
    restart: unless-stopped
    depends_on:
      - ollama
    networks:
      - mcp-network
    # 컨테이너 유지 (VSCode에서 docker exec로 mcp-stdio-server.js 실행)
    command: tail -f /dev/null
    healthcheck:
      test: ["CMD", "test", "-f", "/app/mcp-stdio-server.js"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Cron Scheduler for Auto Scan (Weekly)
  cron-scheduler:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: bestcase-cron-scheduler
    volumes:
      - D:/01.Work/01.Projects:/projects
    environment:
      - NODE_ENV=production
      - PROJECTS_PATH=/projects
      - BESTCASE_STORAGE_PATH=/projects/.bestcases
      - OLLAMA_URL=http://ollama:11434
      - LLM_MODEL=qwen2.5-coder:7b
      - CONCURRENCY=2
    deploy:
      resources:
        limits:
          memory: 4G
    restart: unless-stopped
    depends_on:
      - ollama
      - mcp-code-mode
    networks:
      - mcp-network
    entrypoint: /bin/sh
    command: >
      -c "
      mkdir -p /var/log &&
      touch /var/log/cron.log &&
      chmod +x /app/scripts/scan/init-scan.sh &&
      chmod +x /app/cron-scan.sh &&
      sh /app/scripts/scan/init-scan.sh >> /var/log/cron.log 2>&1 &&
      echo '0 2 * * 0 /app/cron-scan.sh >> /var/log/cron.log 2>&1' | crontab - &&
      cron &&
      echo 'Initialization complete. Weekly cron scheduler started (Every Sunday at 2:00 AM)' &&
      echo 'Next run: Sunday 02:00' &&
      tail -f /var/log/cron.log
      "

  # 개발용 서비스 (선택적)
  mcp-code-mode-dev:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: mcp-code-mode-dev
    ports:
      - "3001:3000"
    volumes:
      - .:/app
      - /app/node_modules
      - D:/01.Work/01.Projects:/projects
    environment:
      - NODE_ENV=development
      - PROJECTS_PATH=/projects
      - BESTCASE_STORAGE_PATH=/projects/.bestcases
      - OLLAMA_URL=http://ollama:11434
    command: yarn workspace web dev
    depends_on:
      - ollama
    networks:
      - mcp-network
    profiles:
      - dev

networks:
  mcp-network:
    driver: bridge

volumes:
  ollama-models:
    driver: local
